{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_g_colab_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wdase/AI-and-ML-projects/blob/master/NLP_g_colab_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mShJMVf84GcW",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing Lab Practice using Python on Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdcMZLutBnIJ",
        "colab_type": "text"
      },
      "source": [
        "Tokenize sentences and words, remove stopwords, use stemmer & lemmatizer\n",
        "\n",
        "**Tokenisation** - Splitting bigger parts to small parts. We can tokenize paragraphs to sentences and sentences to words. The process of converting the normal text strings into a list of tokens (words that we actually want).\n",
        "\n",
        "**Stemming** - Removing affixes from words and returning the root word.\n",
        "\n",
        "**Lemmatization** - Word lemmatizing is similar to stemming, but the difference lies in the output. The Lemmatized output is a real word and not just any trimmed word. For this piece of code to work, you will have to download the wordnet package for nltk.         \n",
        "\n",
        "> from nltk.stem import WordNetLemmatizer    \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "**Stop words**: There are some words in English like “the,” “of,” “a,” “an,” and so on. These are ‘stop words’. Stop words differ from language to language. These stop words may affect the results and thus removing them is necessary.\n",
        "\n",
        "**Count word frequency** - Counting the frequency of occurrence of a word is a crucial part of language analysis. NLTK ships with a word frequency counter in order to count the number of times the word is repeated in a particular dataset.\n",
        "\n",
        "**Synonyms/Antonyms** - And finally, we can also find Synonyms as well as Antonyms of any English word we desire.\n",
        "\n",
        "**Pos Tagging**: The English language is formed of different parts of speech (POS) like nouns, verbs, pronouns, adjectives, etc. POS tagging analyzes the words in a sentences and associates it with a POS tag depending on the way it is used. Also called grammatical tagging or word-category disambiguation. Use nltk.pos_tag. There are different types of tagsets used with the most common being the Penn Treebank tagset and the Universal tagset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK--g8Os4F9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruPqUWrszGUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "354172b4-04b5-4e5c-9f54-09bf04f85ca1"
      },
      "source": [
        "# Setup\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')     #for stop words\n",
        "nltk.download('wordnet')       #for lemmatizer\n",
        "nltk.download('punkt')         #\n",
        "nltk.download('averaged_perceptron_tagger')       # for POS Tagging\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkuFgqi__mja",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLWRn7km_To3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "EXAMPLE_TEXT = \"Word tokenization is the process of splitting a large sample of text into words. We can also tokenize the sentences in a paragraph like we tokenized the words. We use the method word_tokenize and  sent_tokenize to achieve these.\"\n",
        "print(word_tokenize(EXAMPLE_TEXT))\n",
        "print(sent_tokenize(EXAMPLE_TEXT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf1oPXk__K_w",
        "colab_type": "text"
      },
      "source": [
        "# Stemmer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw_znSda-b9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9290c87-178a-492a-b3b4-af8a7f023c1c"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "word_data = \"In the areas of Natural Language Processing we come across situation where two or more words have a common root. For example, the three words - agreed, agreeing and agreeable have the same root word agree. A search involving any of these words should treat them as the same word which is the root word. So it becomes essential to link all the words into their root word. The NLTK library has methods to do this linking and give the output showing the root word. This program uses the Porter Stemming Algorithm for stemming.\"\n",
        "\n",
        "\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "for w in nltk_tokens:\n",
        "    print (\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: In  Stem: In\n",
            "Actual: the  Stem: the\n",
            "Actual: areas  Stem: area\n",
            "Actual: of  Stem: of\n",
            "Actual: Natural  Stem: natur\n",
            "Actual: Language  Stem: languag\n",
            "Actual: Processing  Stem: process\n",
            "Actual: we  Stem: we\n",
            "Actual: come  Stem: come\n",
            "Actual: across  Stem: across\n",
            "Actual: situation  Stem: situat\n",
            "Actual: where  Stem: where\n",
            "Actual: two  Stem: two\n",
            "Actual: or  Stem: or\n",
            "Actual: more  Stem: more\n",
            "Actual: words  Stem: word\n",
            "Actual: have  Stem: have\n",
            "Actual: a  Stem: a\n",
            "Actual: common  Stem: common\n",
            "Actual: root  Stem: root\n",
            "Actual: .  Stem: .\n",
            "Actual: For  Stem: for\n",
            "Actual: example  Stem: exampl\n",
            "Actual: ,  Stem: ,\n",
            "Actual: the  Stem: the\n",
            "Actual: three  Stem: three\n",
            "Actual: words  Stem: word\n",
            "Actual: -  Stem: -\n",
            "Actual: agreed  Stem: agre\n",
            "Actual: ,  Stem: ,\n",
            "Actual: agreeing  Stem: agre\n",
            "Actual: and  Stem: and\n",
            "Actual: agreeable  Stem: agreeabl\n",
            "Actual: have  Stem: have\n",
            "Actual: the  Stem: the\n",
            "Actual: same  Stem: same\n",
            "Actual: root  Stem: root\n",
            "Actual: word  Stem: word\n",
            "Actual: agree  Stem: agre\n",
            "Actual: .  Stem: .\n",
            "Actual: A  Stem: A\n",
            "Actual: search  Stem: search\n",
            "Actual: involving  Stem: involv\n",
            "Actual: any  Stem: ani\n",
            "Actual: of  Stem: of\n",
            "Actual: these  Stem: these\n",
            "Actual: words  Stem: word\n",
            "Actual: should  Stem: should\n",
            "Actual: treat  Stem: treat\n",
            "Actual: them  Stem: them\n",
            "Actual: as  Stem: as\n",
            "Actual: the  Stem: the\n",
            "Actual: same  Stem: same\n",
            "Actual: word  Stem: word\n",
            "Actual: which  Stem: which\n",
            "Actual: is  Stem: is\n",
            "Actual: the  Stem: the\n",
            "Actual: root  Stem: root\n",
            "Actual: word  Stem: word\n",
            "Actual: .  Stem: .\n",
            "Actual: So  Stem: So\n",
            "Actual: it  Stem: it\n",
            "Actual: becomes  Stem: becom\n",
            "Actual: essential  Stem: essenti\n",
            "Actual: to  Stem: to\n",
            "Actual: link  Stem: link\n",
            "Actual: all  Stem: all\n",
            "Actual: the  Stem: the\n",
            "Actual: words  Stem: word\n",
            "Actual: into  Stem: into\n",
            "Actual: their  Stem: their\n",
            "Actual: root  Stem: root\n",
            "Actual: word  Stem: word\n",
            "Actual: .  Stem: .\n",
            "Actual: The  Stem: the\n",
            "Actual: NLTK  Stem: nltk\n",
            "Actual: library  Stem: librari\n",
            "Actual: has  Stem: ha\n",
            "Actual: methods  Stem: method\n",
            "Actual: to  Stem: to\n",
            "Actual: do  Stem: do\n",
            "Actual: this  Stem: thi\n",
            "Actual: linking  Stem: link\n",
            "Actual: and  Stem: and\n",
            "Actual: give  Stem: give\n",
            "Actual: the  Stem: the\n",
            "Actual: output  Stem: output\n",
            "Actual: showing  Stem: show\n",
            "Actual: the  Stem: the\n",
            "Actual: root  Stem: root\n",
            "Actual: word  Stem: word\n",
            "Actual: .  Stem: .\n",
            "Actual: This  Stem: thi\n",
            "Actual: program  Stem: program\n",
            "Actual: uses  Stem: use\n",
            "Actual: the  Stem: the\n",
            "Actual: Porter  Stem: porter\n",
            "Actual: Stemming  Stem: stem\n",
            "Actual: Algorithm  Stem: algorithm\n",
            "Actual: for  Stem: for\n",
            "Actual: stemming  Stem: stem\n",
            "Actual: .  Stem: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBlPioAX_xI9",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2zTfCax_0ZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b998ce6a-b8b6-4bf7-c776-1807145b6f85"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "word_data = \"Lemmatization is similar to stemming but it brings context to the words. So it goes a steps further by linking words with similar meaning to one word. For example if a paragraph has words like cars, trains and automobile, then it will link all of them to automobile. In the below program we use the WordNet lexical database for lemmatization.\"\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "\n",
        "for w in nltk_tokens:\n",
        "     print (\"Actual: %s  Lemma: %s\"  %    (w,wordnet_lemmatizer.lemmatize(w)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: Lemmatization  Lemma: Lemmatization\n",
            "Actual: is  Lemma: is\n",
            "Actual: similar  Lemma: similar\n",
            "Actual: to  Lemma: to\n",
            "Actual: stemming  Lemma: stemming\n",
            "Actual: but  Lemma: but\n",
            "Actual: it  Lemma: it\n",
            "Actual: brings  Lemma: brings\n",
            "Actual: context  Lemma: context\n",
            "Actual: to  Lemma: to\n",
            "Actual: the  Lemma: the\n",
            "Actual: words  Lemma: word\n",
            "Actual: .  Lemma: .\n",
            "Actual: So  Lemma: So\n",
            "Actual: it  Lemma: it\n",
            "Actual: goes  Lemma: go\n",
            "Actual: a  Lemma: a\n",
            "Actual: steps  Lemma: step\n",
            "Actual: further  Lemma: further\n",
            "Actual: by  Lemma: by\n",
            "Actual: linking  Lemma: linking\n",
            "Actual: words  Lemma: word\n",
            "Actual: with  Lemma: with\n",
            "Actual: similar  Lemma: similar\n",
            "Actual: meaning  Lemma: meaning\n",
            "Actual: to  Lemma: to\n",
            "Actual: one  Lemma: one\n",
            "Actual: word  Lemma: word\n",
            "Actual: .  Lemma: .\n",
            "Actual: For  Lemma: For\n",
            "Actual: example  Lemma: example\n",
            "Actual: if  Lemma: if\n",
            "Actual: a  Lemma: a\n",
            "Actual: paragraph  Lemma: paragraph\n",
            "Actual: has  Lemma: ha\n",
            "Actual: words  Lemma: word\n",
            "Actual: like  Lemma: like\n",
            "Actual: cars  Lemma: car\n",
            "Actual: ,  Lemma: ,\n",
            "Actual: trains  Lemma: train\n",
            "Actual: and  Lemma: and\n",
            "Actual: automobile  Lemma: automobile\n",
            "Actual: ,  Lemma: ,\n",
            "Actual: then  Lemma: then\n",
            "Actual: it  Lemma: it\n",
            "Actual: will  Lemma: will\n",
            "Actual: link  Lemma: link\n",
            "Actual: all  Lemma: all\n",
            "Actual: of  Lemma: of\n",
            "Actual: them  Lemma: them\n",
            "Actual: to  Lemma: to\n",
            "Actual: automobile  Lemma: automobile\n",
            "Actual: .  Lemma: .\n",
            "Actual: In  Lemma: In\n",
            "Actual: the  Lemma: the\n",
            "Actual: below  Lemma: below\n",
            "Actual: program  Lemma: program\n",
            "Actual: we  Lemma: we\n",
            "Actual: use  Lemma: use\n",
            "Actual: the  Lemma: the\n",
            "Actual: WordNet  Lemma: WordNet\n",
            "Actual: lexical  Lemma: lexical\n",
            "Actual: database  Lemma: database\n",
            "Actual: for  Lemma: for\n",
            "Actual: lemmatization  Lemma: lemmatization\n",
            "Actual: .  Lemma: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaUeclDvAJzk",
        "colab_type": "text"
      },
      "source": [
        "# POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOA85RB-AN1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ca95807-43fb-4c3b-e53c-6c415193c7b9"
      },
      "source": [
        "import nltk\n",
        "text=nltk.word_tokenize(\"And now for something compeletely\")\n",
        "print(nltk.pos_tag(text))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('compeletely', 'RB')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}